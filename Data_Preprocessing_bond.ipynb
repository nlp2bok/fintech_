{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/nlp2bok/fintech_/blob/lmh_dev/Data_Preprocessing_bond.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VxOdSCvqxtLv"
   },
   "source": [
    "#Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nPPNzK3o9R13"
   },
   "source": [
    "##. 1. 채권분석 보고서(Bond Analysis Report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b1l-yKymyDhE"
   },
   "source": [
    "1. 채권보고서 파일명(cName_rDate.txt)\n",
    "\n",
    " 1495개의 채권보고서를 하나의 csv파일에 넣는다.\n",
    "  파일명에서 cName, rDate추출, 파일 read를 통한 내용 추출('회사명','작성일자','내용')\n",
    "2. 다양한 Tokenized 방식을 통해 전처리를 실행한다.\n",
    "\n",
    " Komoran, Hannanum,  Okt, Kkma, Mecab => Mecab사용\n",
    " \n",
    " -> Mecab install 필요\n",
    "3. Token에 한글 태그를 부착한다.\n",
    "4. 불용어 처리를 실행한다.\n",
    "5. csv파일에 preprocessing data 컬럼을 추가한다(preData)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O5ypPCidxsTt"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dUyIHOSLvZ0O"
   },
   "source": [
    "##Step 1. 전체 파일을 하나의 파일로 합치기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BPZF9aO4vZ0P"
   },
   "outputs": [],
   "source": [
    "#print(dirpath) #현재경로 출력\n",
    "workDIr = os.path.abspath('/Users/minhyeon/Desktop/BondReport_Text/bText/')\n",
    "cnt = 0\n",
    "\n",
    "def fileopen(path):\n",
    "    f = open(path,'r', encoding='utf-8')\n",
    "    line = f.read()\n",
    "    f.close()\n",
    "    \n",
    "    return line \n",
    "\n",
    "with open('BondFile.csv', 'w', encoding='utf-8', newline='') as cvsFile :\n",
    "    wr = csv.writer(cvsFile)\n",
    "\n",
    "    for dirpath, dirnames, filenames in os.walk(workDIr):\n",
    "        for filename in filenames:\n",
    "            #파일 컨텐츠 읽기\n",
    "            path = dirpath+'/'+filename\n",
    "            fType = filename[-4:]\n",
    "            try:\n",
    "                if fType == \".txt\" : \n",
    "                    text = fileopen(path)\n",
    "                else :\n",
    "                    continue\n",
    "                #파일 타입 체크\n",
    "            except Exception as ex:\n",
    "                print(filename + '파일 read 에러가발생 했습니다', ex)\n",
    "\n",
    "\n",
    "            try:\n",
    "                rDate = \"20\" + filename[-12:-4].replace('.','') #Report Date, String형\n",
    "                cName = filename[:-13] #Company Name\n",
    "                #print(cnt)\n",
    "                #print(cName)\n",
    "                cnt += 1\n",
    "                wr.writerow([cName,rDate,text])\n",
    "            except Exception as ex:\n",
    "                print(filename + '에러가 발생 했습니다', ex)\n",
    "    \n",
    "cvsFile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JTC4jIpYvZ0U"
   },
   "source": [
    "## Step2. Tokenized 실행"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hzkNYHT-wddh"
   },
   "source": [
    "##2-1. 파일 읽어오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lqfLDCnRvZ0V"
   },
   "outputs": [],
   "source": [
    "#csv파일을 읽어오고 column명을 설정해준다.\n",
    "mydata = pd.read_csv(\"BondFile.csv\",names=['cName','rDate','bContent'],encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GdVGaaAPAlS1"
   },
   "outputs": [],
   "source": [
    "#읽어온 데이터를 DataFrame형식으로 담는다.\n",
    "df = pd.DataFrame(mydata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G7q8KopB5icf",
    "outputId": "e4e339b4-28dc-4bde-c840-08883d0b2215"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1494\n"
     ]
    }
   ],
   "source": [
    "#323, 410, 870 error\n",
    "#mycontent에 전처리할 데이터만 담는다.\n",
    "mycontent = df['bContent']\n",
    "print(len(mycontent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vdcS1w3k4vtG"
   },
   "source": [
    "## 2-2. 다양한 방식의 Tokenized 활용해보기\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eKTFHt0kwil1"
   },
   "source": [
    "기사별 Tokenized한 값을 komoran_tokens, hannanum_tokens, okt_tokens, kkma_tokens, Mecab_tokens로 각각 저장한 후 결과를 확인해본다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jNj0GcqU5TY1"
   },
   "outputs": [],
   "source": [
    "#출력을 위해서 임시 class \n",
    "class List(list): \n",
    "    def __str__(self): \n",
    "        return \"[\" + \", \".join([\"%s\" % x for x in self]) + \"]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XOZ8VBWLLWoH"
   },
   "outputs": [],
   "source": [
    "#코모란(Komoran) 토큰화\n",
    "from konlpy.tag import Komoran\n",
    "komoran = Komoran()\n",
    "komoran_tokens = []\n",
    "for i in range(len(mycontent)):\n",
    "    try:\n",
    "        komoran_tokens.append(komoran.morphs(mycontent[i]))\n",
    "        print(i)\n",
    "    except:\n",
    "        #이상 데이터 제외\n",
    "        print(i)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RB31gCTx4kmk"
   },
   "outputs": [],
   "source": [
    "#한나눔(Hannanum) 토큰화\n",
    "from konlpy.tag import Hannanum\n",
    "hannanum = Hannanum()\n",
    "hannanum_tokens = []\n",
    "for i in range(len(mycontent)):\n",
    "    try:\n",
    "        hannanum_tokens.append(hannanum.morphs(mycontent[i]))\n",
    "    except:\n",
    "        #이상 데이터 제외\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oaiOQqJrw3_x"
   },
   "outputs": [],
   "source": [
    "#Okt 토큰화\n",
    "from konlpy.tag import Okt\n",
    "okt =Okt()\n",
    "okt_tokens = []\n",
    "for i in range(len(mycontent)):\n",
    "    try:\n",
    "        okt_tokens.append(okt.morphs(mycontent[i]))\n",
    "    except:\n",
    "        #이상 데이터 제외\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_dLEBr77w4wS"
   },
   "outputs": [],
   "source": [
    "#Kkma 토큰화\n",
    "from konlpy.tag import Kkma\n",
    "kkma = Kkma()\n",
    "kkma_tokens = []\n",
    "for i in range(len(mycontent)):\n",
    "    try:\n",
    "        kkma_tokens.append(kkma.morphs(mycontent[i]))\n",
    "    except:\n",
    "        #이상 데이터 제외\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1_eCgXytvZ0q"
   },
   "outputs": [],
   "source": [
    "#MeCab 토큰화\n",
    "from ekonlpy.tag import Mecab\n",
    "mecab = Mecab()\n",
    "mecab_tokens = []\n",
    "for i in range(len(mycontent)):\n",
    "    try:\n",
    "        #print(mecab.pos(mycontent[i]))\n",
    "        #break\n",
    "        mecab_tokens.append(mecab.morphs(mycontent[i]))\n",
    "        #print(i)\n",
    "    except:\n",
    "        #이상 데이터 제외\n",
    "        print(str(i)+\"error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8Jb1y_BZ5ic0"
   },
   "outputs": [],
   "source": [
    "#토큰화된 데이터 중 첫번째 데이터 확인\n",
    "print(mecab_tokens[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "besapcdgxV4G"
   },
   "source": [
    "##Step3. Mecab을 사용하여 한글 태그 부착\n",
    "가장 성능이 좋은 Mecab을 활용하기로 하였음.\n",
    "\n",
    "한글 태그 부착은 .pos를 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t3gYVUdc5ic2",
    "outputId": "94ec2fb3-5a3e-41ef-e80f-84417aa82eaa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "323error\n",
      "410error\n",
      "870error\n"
     ]
    }
   ],
   "source": [
    "#MeCab 토큰화(ekonlpy를 통한 한글 태그 부착 -> ekonlpy 설치 필요)\n",
    "from ekonlpy.tag import Mecab\n",
    "mecab = Mecab()\n",
    "mecab_tokens = []\n",
    "for i in range(len(mycontent)):\n",
    "    try:\n",
    "        #저작권삭제\n",
    "        mecab_tokens.append(mecab.pos(mycontent[i]))\n",
    "        #print(i)\n",
    "    except:\n",
    "        #이상 데이터 제외\n",
    "        print(str(i)+'error')\n",
    "        #이상한 데이터에는 빈 값을 넣어준다.\n",
    "        mecab_tokens.append(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G9nISBPF5ic6",
    "outputId": "7e6b921d-556e-4ac5-a692-7f560ae835e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    }
   ],
   "source": [
    "print(mecab_tokens[323])\n",
    "#채권보고서1 데이터 토큰화\n",
    "#print(mecab_tokens[870])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iIi12uUOvZ0z"
   },
   "outputs": [],
   "source": [
    "#한글태그 부착확인\n",
    "#print(mecab_tokens[0][1])\n",
    "#mecab_tokens[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gAmmDv7w5idA",
    "outputId": "53b7bf59-a2c2-4b8d-89ce-eee5c45c26fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1494\n",
      "1494\n"
     ]
    }
   ],
   "source": [
    "#데이터 개수 확인\n",
    "print(len(mycontent))\n",
    "print(len(mecab_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ID4ZFMogx8ny"
   },
   "source": [
    "##Step4. 불용어 처리\n",
    "\n",
    "(1) stopPos에서는 불필요한 형태소를 한번에 제거\n",
    "\n",
    "(2) stopWord는 불필요한 단어를 수동으로 제거"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UN0zCYIM5idI"
   },
   "source": [
    "mecab-ko-dic 품사태그\n",
    "\n",
    "명사(NNG), 형용사(VA,VAX), 부사(MAG), 동사(VA)만 추출\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nFW3FGgt5idJ"
   },
   "outputs": [],
   "source": [
    "##명사(NNG), 형용사(VA,VAX), 부사(MAG), 동사(VA)\n",
    "includePos = ['NNG','VA','VAX','MAG','VA']\n",
    "stopWord = [\"Microsoft\"]\n",
    "result =[]\n",
    "preresult = []\n",
    "cnt = 0\n",
    "#1493개의 mecab_tokens loop\n",
    "for mecab_token in mecab_tokens :\n",
    "    #print(mecab_token)\n",
    "    #mecab_token(i)번째의 (word/형태소) 데이터에 접근 => wordData = ('단어','형태소')\n",
    "    cnt+=1\n",
    "    #print(cnt)\n",
    "    for wordData in mecab_token:\n",
    "        #('단어','형태소')중 '형태소' 데이터가 stopPos, '단어' 데이터가 stopWord에 포함되지 않으면 저장한다.\n",
    "        try:\n",
    "            if wordData[1] in includePos:\n",
    "                if wordData[0] not in stopWord:\n",
    "                    preresult.append(wordData[0])\n",
    "        except:\n",
    "            continue\n",
    "    result.append(','.join(preresult))\n",
    "    preresult = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "saZhh7-39Ied"
   },
   "source": [
    "##Step5. 전처리된 데이터를 기존 파일에 추가한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ftj3L3JV5idN"
   },
   "outputs": [],
   "source": [
    "#Question3. 데이터 넣는방법 질문...\n",
    "#print(len(result))\n",
    "df['preData'] = result\n",
    "print(df['preData'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RYX_V_uu5idQ"
   },
   "outputs": [],
   "source": [
    "\n",
    "df.to_csv(\"/Users/minhyeon/Desktop/BondReport_Text/bond_final.csv\", header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t2m1Vgje5idT"
   },
   "outputs": [],
   "source": [
    "text = pd.read_csv(\"bond_final.csv\",encoding=\"utf-8\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Data_Preprocessing_bond.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
