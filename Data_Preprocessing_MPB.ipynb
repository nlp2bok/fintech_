{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/nlp2bok/fintech_/blob/lmh_dev/Data_Preprocessing_MPB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8krwcoAYBEJd"
   },
   "source": [
    "#Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Fl0c3I6vBEJe"
   },
   "source": [
    "##. 1. 금통위 의사록(MPB Analysis Report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N63xEVpaBEJf"
   },
   "source": [
    "1. MPB 데이터 병합\n",
    "\n",
    " 160개의 금통위 의사록을 하나의 csv파일에 넣는다.\n",
    "  파일명에서 rDate추출, 파일 read를 통한 내용 추출('작성일자','내용')\n",
    "2. 다양한 Tokenized 방식을 통해 전처리를 실행한다.\n",
    "\n",
    " Komoran, Hannanum,  Okt, Kkma, Mecab => Mecab사용\n",
    " \n",
    " -> Mecab install 필요\n",
    "3. Token에 한글 태그를 부착한다.\n",
    "4. 불용어 처리를 실행한다.\n",
    "5. csv파일에 preprocessing data 컬럼을 추가한다(preData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "awIygVG5BEJg"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rwixaBr1BEJj"
   },
   "source": [
    "##Step 1. 전체 파일을 하나의 파일로 합치기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VQMaqrEgBEJk"
   },
   "outputs": [],
   "source": [
    "#print(dirpath) #현재경로 출력\n",
    "workDIr = os.path.abspath('/Users/minhyeon/Desktop/Data_Preprocessing_MPB/MPB_Contents/')\n",
    "cnt = 0\n",
    "\n",
    "def fileopen(path):\n",
    "    f = open(path,'r', encoding='utf-8')\n",
    "    line = f.read()\n",
    "    f.close()\n",
    "    \n",
    "    return line \n",
    "\n",
    "with open('MPB_File.csv', 'w', encoding='utf-8', newline='') as cvsFile :\n",
    "    wr = csv.writer(cvsFile)\n",
    "\n",
    "    for dirpath, dirnames, filenames in os.walk(workDIr):\n",
    "        for filename in filenames:\n",
    "            #파일 컨텐츠 읽기\n",
    "            path = dirpath+'/'+filename\n",
    "            fType = filename[-4:]\n",
    "            try:\n",
    "                if fType == \".txt\" : \n",
    "                    text = fileopen(path)\n",
    "                else :\n",
    "                    continue\n",
    "                #파일 타입 체크\n",
    "            except Exception as ex:\n",
    "                print(filename + '파일 read 에러가발생 했습니다', ex)\n",
    "\n",
    "\n",
    "            try:\n",
    "                rDate = filename[:4] #Report Date, String형\n",
    "                #print(cnt)\n",
    "                cnt += 1\n",
    "                wr.writerow([rDate,text])\n",
    "            except Exception as ex:\n",
    "                print(filename + '에러가 발생 했습니다', ex)\n",
    "    \n",
    "cvsFile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nyyZwD6jBEJm"
   },
   "source": [
    "## Step2. Tokenized 실행"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wBTGz34ABEJm"
   },
   "source": [
    "##2-1. 파일 읽어오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6n5PMpBsBEJn"
   },
   "outputs": [],
   "source": [
    "#csv파일을 읽어오고 column명을 설정해준다.\n",
    "mydata = pd.read_csv(\"MPB_File.csv\",names=['rDate','MPBcontent'],encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0HLmQyqkBEJp"
   },
   "outputs": [],
   "source": [
    "#읽어온 데이터를 DataFrame형식으로 담는다.\n",
    "df = pd.DataFrame(mydata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PgOLrF4uBEJr",
    "outputId": "9fd19dfc-4011-4ddb-8722-e8795e0bc3ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160\n"
     ]
    }
   ],
   "source": [
    "#mycontent에 전처리할 데이터만 담는다.\n",
    "mycontent = df['MPBcontent']\n",
    "print(len(mycontent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bVnfS1ebBEJv"
   },
   "source": [
    "## 2-2. 다양한 방식의 Tokenized 활용해보기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Hp1LvrH0BEJw"
   },
   "source": [
    "기사별 Tokenized한 값을 komoran_tokens, hannanum_tokens, okt_tokens, kkma_tokens, Mecab_tokens로 각각 저장한 후 결과를 확인해본다.\n",
    "\n",
    "-> 채권보고서에서 Mecab성능이 가장 좋은 것을 확인하였음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NO4X7Kp8BEJw"
   },
   "outputs": [],
   "source": [
    "#MeCab 토큰화\n",
    "from ekonlpy.tag import Mecab\n",
    "mecab = Mecab()\n",
    "mecab_tokens = []\n",
    "for i in range(len(mycontent)):\n",
    "    try:\n",
    "        #print(mecab.pos(mycontent[i]))\n",
    "        #break\n",
    "        mecab_tokens.append(mecab.morphs(mycontent[i]))\n",
    "        #print(i)\n",
    "    except:\n",
    "        #이상 데이터 제외\n",
    "        print(str(i)+\"error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iQerbYolBEJy"
   },
   "source": [
    "##Step3. Mecab을 사용하여 한글 태그 부착\n",
    "가장 성능이 좋은 Mecab을 활용하기로 하였음.\n",
    "\n",
    "한글 태그 부착은 .pos를 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EbmOA8oFBEJy"
   },
   "outputs": [],
   "source": [
    "#MeCab 토큰화(ekonlpy를 통한 한글 태그 부착 -> ekonlpy 설치 필요)\n",
    "from ekonlpy.tag import Mecab\n",
    "mecab = Mecab()\n",
    "mecab_tokens = []\n",
    "for i in range(len(mycontent)):\n",
    "    try:\n",
    "        mecab_tokens.append(mecab.pos(mycontent[i]))\n",
    "        #print(i)\n",
    "    except:\n",
    "        #이상 데이터 제외\n",
    "        print(str(i)+'error')\n",
    "        #이상한 데이터에는 빈 값을 넣어준다.\n",
    "        mecab_tokens.append(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PFLjCYjFBEJ0",
    "outputId": "5ff087e9-7359-4e97-f166-7322af9d96a8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('년', 'NNBC')\n",
      "160\n",
      "160\n"
     ]
    }
   ],
   "source": [
    "#한글태그 부착확인\n",
    "print(mecab_tokens[0][1])\n",
    "print(mecab_tokens[0])\n",
    "\n",
    "#데이터 개수 확인\n",
    "print(len(mycontent))\n",
    "print(len(mecab_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i5phbvssBEJ3"
   },
   "source": [
    "##Step4. 불용어 처리\n",
    "\n",
    "(1) stopInclude에 포함된 품사만 남긴다.\n",
    "\n",
    "(2) stopWord는 불필요한 단어를 수동으로 제거"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jGnfgbVVBEJ3"
   },
   "source": [
    "mecab-ko-dic 품사태그\n",
    "\n",
    "명사(NNG), 형용사(VA,VAX), 부사(MAG), 동사(VA)만 추출\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f7IclsYCBEJ4"
   },
   "outputs": [],
   "source": [
    "##명사(NNG), 형용사(VA,VAX), 부사(MAG), 동사(VA)\n",
    "includePos = ['NNG','VA','VAX','MAG','VA']\n",
    "stopWord = [\"덤프데이터\"]\n",
    "result =[]\n",
    "preresult = []\n",
    "cnt = 0\n",
    "#1493개의 mecab_tokens loop\n",
    "for mecab_token in mecab_tokens :\n",
    "    #print(mecab_token)\n",
    "    #mecab_token(i)번째의 (word/형태소) 데이터에 접근 => wordData = ('단어','품사')\n",
    "    cnt+=1\n",
    "    #print(cnt)\n",
    "    for wordData in mecab_token:\n",
    "        #('단어','품사')중 '형태소' 데이터가 stopInclude에 포함, '단어' 데이터가 stopWord에 포함되지 않으면 저장한다.\n",
    "        try:\n",
    "            if wordData[1] in includePos:\n",
    "                if wordData[0] not in stopWord:\n",
    "                    preresult.append(wordData[0])\n",
    "        except:\n",
    "            continue\n",
    "    result.append(','.join(preresult))\n",
    "    preresult = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uhQkrecZBEJ6"
   },
   "source": [
    "##Step5. 전처리된 데이터를 기존 파일에 추가한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QmMfKS6xBEJ6"
   },
   "outputs": [],
   "source": [
    "#print(len(result))\n",
    "df['preData'] = result\n",
    "print(df['preData'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5gb60jnUBEJ9"
   },
   "outputs": [],
   "source": [
    "\n",
    "df.to_csv(\"/Users/minhyeon/Desktop/Data_Preprocessing_MPB/MPB_final.csv\", header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hFcOFJiyBEJ_"
   },
   "outputs": [],
   "source": [
    "text = pd.read_csv(\"MPB_final.csv\",encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k4_QC7uBBEKC"
   },
   "outputs": [],
   "source": [
    "print(text['preData'][0])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": "Data_Preprocessing_MPB.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
